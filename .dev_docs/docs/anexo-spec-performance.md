Perfeito. Vamos formalizar esses detalhes t√©cnicos.

Este documento complementa o **DOC 00**, focando especificamente nas configura√ß√µes de implementa√ß√£o (Quantiza√ß√£o e Lat√™ncia), servindo como guia de configura√ß√£o para o momento do _deploy_.

---

# ANEXO T√âCNICO: Especifica√ß√µes de Implementa√ß√£o e Performance

**Refer√™ncia:** Arquitetura SysVox (Ryzen 7 + GTX 1650 4GB)
**Data:** 21/12/2025

---

## 1. Tabela Mestra de Ferramentas e Configura√ß√µes (A Stack)

Esta tabela define exatamente qual arquivo de modelo baixar, qual n√≠vel de compress√£o (quantiza√ß√£o) usar e onde ele deve rodar.

| Componente        | Modelo Espec√≠fico                        | Engine / Backend            | Quantiza√ß√£o (Precis√£o)           | Aloca√ß√£o de Hardware     | Consumo Estimado |
| ----------------- | ---------------------------------------- | --------------------------- | -------------------------------- | ------------------------ | ---------------- |
| **C√©rebro (LLM)** | **IBM Granite 4.0 Hybrid 1B** (Instruct) | `llama.cpp` (Suporte Mamba) | **Q8_0** (Alta Precis√£o)         | **100% GPU** (VRAM)      | ~1.3 GB VRAM     |
| **Ouvido (STT)**  | **Whisper Large v3**                     | `whisper.cpp`               | **Q5_K_M** (Equil√≠brio Vel/Qual) | **100% CPU** (4 Threads) | ~2.2 GB RAM      |
| **Mem√≥ria (RAG)** | **BAAI BGE-M3**                          | `ONNX Runtime` ou `PyTorch` | **FP16** (Padr√£o)                | **CPU** (Sob demanda)    | ~1.5 GB RAM      |
| **Voz (TTS)**     | **Piper** (pt_BR Custom)                 | `piper-tts`                 | **ONNX High** (Padr√£o)           | **CPU** (1 Thread)       | ~300 MB RAM      |
| **Gatilho**       | **openWakeWord**                         | `tflite` / `onnx`           | **INT8** (Padr√£o)                | **CPU** (Low Priority)   | ~150 MB RAM      |

### üìù Notas de Engenharia:

- **Granite em Q8:** Optamos pela quantiza√ß√£o Q8 (quase sem perda) em vez de Q4 porque o modelo √© pequeno (1B). Com 4GB de VRAM, temos luxo de usar a melhor qualidade poss√≠vel para maximizar a intelig√™ncia.
- **Whisper em Q5:** A vers√£o Q5_K_M do Whisper Large v3 tem perda de precis√£o impercept√≠vel para √°udio, mas roda muito mais leve na CPU que a vers√£o FP16.

---

## 2. Proje√ß√£o de Lat√™ncia (Pipeline de Streaming)

Esta tabela reflete o **"Time-to-First-Audio"** (Tempo at√© o usu√°rio ouvir o primeiro som), considerando o processamento paralelo (Streaming).

**Cen√°rio:** Usu√°rio termina uma frase de comando m√©dio.

| Sequ√™ncia | A√ß√£o do Sistema              | Dura√ß√£o (ms) | Status do Processamento                                                  |
| --------- | ---------------------------- | ------------ | ------------------------------------------------------------------------ |
| **T0**    | Usu√°rio para de falar.       | 0 ms         | O Whisper j√° processou 95% da frase durante a fala.                      |
| **T1**    | **VAD (Janela de Sil√™ncio)** | +300 ms      | Sistema aguarda para confirmar que n√£o √© apenas uma pausa para respirar. |
| **T2**    | **Whisper (Finaliza√ß√£o)**    | +50 ms       | Processa os √∫ltimos tokens e pontua√ß√£o final.                            |
| **T3**    | **RAG (Busca Vetorial)**     | +30 ms       | BGE-M3 busca o contexto na RAM (instru√ß√£o matem√°tica r√°pida).            |
| **T4**    | **LLM (Primeiro Token)**     | +50 ms       | Granite (na GPU) gera a primeira palavra da resposta.                    |
| **T5**    | **Piper (Buffer Inicial)**   | +100 ms      | Gera o √°udio da primeira frase/trecho e envia para a sa√≠da.              |
| **TOTAL** | **Lat√™ncia Percebida**       | **~530 ms**  | **Sensa√ß√£o de conversa natural fluida.**                                 |

---

## 3. Gest√£o de Mem√≥ria da GPU (O "Fit" na GTX 1650)

C√°lculo de seguran√ßa para garantir que o sistema n√£o trave por falta de mem√≥ria de v√≠deo (OOM - Out of Memory).

- **VRAM Total Dispon√≠vel:** 4096 MB (4 GB)
- **Reserva do Sistema (Windows/Monitor):** -600 MB
- **Modelo Granite 4.0 (Q8_0):** -1300 MB
- **Contexto KV Cache (Mamba-2 Linear):** -300 MB (Mesmo com textos longos)
- **Buffers de Infer√™ncia (CUDA):** -400 MB
- **Margem de Seguran√ßa (Livre):** **~1.5 GB**

**Conclus√£o:** O sistema roda com "folga" na GPU, garantindo que n√£o haver√° _swapping_ para a RAM (o que mataria a performance).

---

Este documento est√° salvo e registrado. Podemos prosseguir com o **DOC 02 (Scripting e Minera√ß√£o de Dados)** agora?
